import streamlit as st
import datetime
import os  # æ–°å¢: ç”¨äºè¯»å–ç¯å¢ƒå˜é‡
# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ¨¡å—
import utils
import arxiv_api
import paper_reader
import ai_agent

# ==========================================
# é¡µé¢é…ç½®
# ==========================================
st.set_page_config(page_title="ArXiv è®ºæ–‡å°åŠ©æ‰‹", page_icon="ğŸ“‘", layout="wide")

# ==========================================
# ä¾§è¾¹æ è®¾ç½®
# ==========================================
st.sidebar.header("ğŸ” æœç´¢è®¾ç½®")

# --- æ–°å¢: é¢„è®¾å…³é”®è¯å­—å…¸ (ä¼˜åŒ–é€šç”¨ç‰ˆ) ---
search_presets = {
    "1. AI + Economics (AIä¸ç»æµ/é‡‘è - é€šç”¨ç‰ˆ)":
        '(Economic OR Economics OR Finance OR Financial OR Market OR "Behavioral Economics") AND (LLM OR "Large Language Model" OR RL OR "Reinforcement Learning" OR "Generative AI")',
    
    "2. Agents (LLM Agent / RL Agent / Multi-Agent)":
        '("Multi-Agent" OR "Multiagent" OR "Autonomous Agent" OR "LLM Agent" OR "Language Agent" OR "RL Agent" OR "Agentic") AND (LLM OR "Large Language Model" OR RL OR "Reinforcement Learning")',
    
    "3. World Models (ä¸–ç•Œæ¨¡å‹ & MBRL)":
        '"World Model" OR "World Models" OR "Generative World Model" OR "Model-Based RL" OR MBRL OR "Predictive Model"',
    
    "4. è‡ªå®šä¹‰ (ç©ºç™½)": ""
}

# é¢„è®¾é€‰æ‹©å™¨
selected_preset_key = st.sidebar.selectbox("å¿«é€Ÿé€‰æ‹©é¢„è®¾", options=list(search_presets.keys()), index=0)
default_keyword = search_presets[selected_preset_key]

# 1. å…³é”®è¯è¾“å…¥æ¡† (value ä¼šæ ¹æ®é¢„è®¾è‡ªåŠ¨å˜åŒ–)
keywords = st.sidebar.text_input("å…³é”®è¯ (æ”¯æŒ AND, OR)", value=default_keyword)

# 2. é¢†åŸŸé€‰æ‹©
category_bundle = st.sidebar.selectbox("é€‰æ‹©æœç´¢èŒƒå›´", options=list(utils.CATEGORY_QUERIES.keys()), index=0)

# 3. æ—¶é—´ä¸æ•°é‡
days_back = st.sidebar.slider("æœç´¢è¿‡å»å¤šå°‘å¤©?", min_value=1, max_value=365, value=7)
max_results = st.sidebar.number_input("æœ€å¤§ç»“æœæ•°é‡", min_value=5, max_value=100, value=20)

st.sidebar.divider()
st.sidebar.header("ğŸ¤– AI è®¾ç½®")

# --- è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å– API Key ---
env_api_key = os.getenv("OPENAI_API_KEY", "")

api_key = st.sidebar.text_input(
    "API Key",
    value=env_api_key,
    type="password",
    help="å¦‚æœåœ¨ç»ˆç«¯è®¾ç½®äº† export OPENAI_API_KEY='...', æ­¤å¤„ä¼šè‡ªåŠ¨å¡«å……"
)

base_url = st.sidebar.text_input("Base URL", value="https://api.openai.com/v1", help="å›½å†…è¯·å¡«è½¬å‘åœ°å€")
model_name = st.sidebar.text_input("æ¨¡å‹åç§°", value="gpt-4o-mini", help="ä¾‹å¦‚ gpt-4o-mini æˆ– deepseek-chat")

# ==========================================
# ä¸»é€»è¾‘
# ==========================================
st.title("ğŸ“‘ ArXiv Paper Daily Tracker")
st.caption(f"å½“å‰æ¨¡å¼: {category_bundle} | çª—å£: {days_back}å¤©")

# åˆå§‹åŒ– Session State
if "papers" not in st.session_state:
    st.session_state.papers = []
if "summaries" not in st.session_state:
    st.session_state.summaries = {}

# --- æœç´¢æŒ‰é’® ---
if st.button("å¼€å§‹æŠ“å–", type="primary"):
    if not keywords:
        st.warning("è¯·è¾“å…¥å…³é”®è¯ï¼")
    else:
        with st.spinner('æ­£åœ¨è¿æ¥ ArXiv æ•°æ®åº“...'):
            # 1. æ„å»ºæŸ¥è¯¢
            query_string = utils.build_query(keywords, category_bundle)
            
            # 2. æ‰§è¡Œæœç´¢
            papers = arxiv_api.fetch_arxiv_papers(query_string, days_back, max_results)
            
            if not papers:
                st.info("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œè¯·å°è¯•æ”¾å®½æ—¶é—´æˆ–æ›´æ¢å…³é”®è¯ã€‚")
            else:
                # 3. ç»“æœå­˜å…¥ Session
                st.session_state.papers = papers
                st.session_state.summaries = {}  # æ¸…ç©ºæ—§æ€»ç»“
                st.success(f"æˆåŠŸæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼")

# --- ç»“æœå±•ç¤º ---
if st.session_state.papers:
    st.divider()
    
    for i, paper in enumerate(st.session_state.papers):
        with st.container():
            col1, col2 = st.columns([0.82, 0.18])
            
            with col1:
                st.subheader(f"{i + 1}. {paper.title}")
                authors = [a.name for a in paper.authors]
                author_str = ", ".join(authors[:3]) + (" et al." if len(authors) > 3 else "")
                
                html_link = f"https://arxiv.org/html/{paper.entry_id.split('/')[-1]}"
                st.markdown(f"**âœï¸ ä½œè€…:** {author_str} | **ğŸ“… å‘å¸ƒ:** {paper.published.strftime('%Y-%m-%d')}")
                st.markdown(f"[HTML é˜…è¯»]({html_link}) | [PDF ä¸‹è½½]({paper.pdf_url}) | [ArXiv Page]({paper.entry_id})")
            
            with col2:
                btn_key = f"ai_btn_{paper.entry_id}"
                if st.button("ğŸ¤– AI å…¨æ–‡è§£è¯»", key=btn_key):
                    status = st.empty()
                    status.info("â³ æ­£åœ¨è·å–æ­£æ–‡ (HTMLä¼˜å…ˆ)...")
                    
                    # 1. è·å–æ­£æ–‡
                    content, src_type = paper_reader.get_paper_content(paper.entry_id, paper.pdf_url)
                    
                    if content.startswith("Error"):
                        status.error("âŒ è·å–å†…å®¹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
                    else:
                        status.info(f"âœ… è·å–æˆåŠŸ ({src_type})! AI æ­£åœ¨é˜…è¯»...")
                        # 2. AI æ€»ç»“
                        summary = ai_agent.get_ai_summary(content, paper.title, api_key, base_url, model_name)
                        
                        # 3. å­˜ç»“æœ
                        st.session_state.summaries[paper.entry_id] = summary
                        status.empty()
            
            if paper.entry_id in st.session_state.summaries:
                st.markdown("#### ğŸ“ AI æ·±åº¦åˆ†ææŠ¥å‘Š")
                st.info(st.session_state.summaries[paper.entry_id])
            
            with st.expander("ğŸ“– æŸ¥çœ‹åŸå§‹æ‘˜è¦ (Abstract)"):
                st.write(paper.summary)
            
            st.divider()
    
    # --- å¯¼å‡ºåŠŸèƒ½ ---
    st.header("ğŸ“¤ å¯¼å‡ºç»“æœ")
    if st.button("ç”Ÿæˆ Markdown æŠ¥å‘Š"):
        export_text = utils.generate_export_text(st.session_state.papers, keywords)
        st.download_button(
            label="ä¸‹è½½æ–‡ä»¶",
            data=export_text,
            file_name=f"arxiv_report_{datetime.datetime.now().strftime('%Y%m%d')}.md",
            mime="text/markdown"
        )